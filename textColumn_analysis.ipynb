{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4c4ebd",
   "metadata": {},
   "source": [
    "# Text column analysis\n",
    "\n",
    "Is it important to be able to group rows (data) by labels. To this purpose, one must make sure that the labels are correct and are not null, mispelled, or written differently. Even more subtile, it should be checked that two different labels do not actually refer to the same concept.\n",
    "\n",
    "_<u>Ex</u>: sod and sodium both refer to sodium, $mm^3$ and $\\mu L$ both refer to microliters_.\n",
    "\n",
    "To facilitate labelling, data processing also implies label checking. \n",
    "\n",
    "In order to help this process, I have written 3 functions that aim at flagging errors and potential typos that one should take into account when applying ML algorithms. \n",
    "\n",
    "Once these generalized functions written, I was able to apply them to my data without modifying the function it-self. \n",
    "\n",
    "**For all three functions, the input data should be passed as a dataframe. The relevant column should contain strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df193d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import fuzz # for computing levenshtein ratios\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter # for letter distribution in a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fc3d2",
   "metadata": {},
   "source": [
    "## Errors in letter capitalization\n",
    "\n",
    "Looking at the lettercase is one of the first step of label checking, and the use of algorithms often implies to give all labels the same capitalization to simplify the process. This means that the rows with different label capitalization should then be grouped under the same label. \n",
    "\n",
    "<u>The **countCapitalization** function:</u>\n",
    "\n",
    "The function takes as input:\n",
    "- a dataframe of elements in the relevant feature column \n",
    "- a column name \n",
    "\n",
    "It returns a dictionary with the following fields:\n",
    "- `initial_count` : the number of distinct elements (of the input)\n",
    "- `decap_count` : the number of distinct elements when removing the capital letters\n",
    "- `diff_distinct_decap` : the number of elements that appear several times but with different capitalizations\n",
    "- `list_df_diff_cap` : a list of all these elements that appear several times with different capitalizations, and said capitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCapitalization(input_dataframe, feature_name) :\n",
    "    \n",
    "    # we remove null values\n",
    "    if (len(dataframe) != len(dataframe.dropna())) :\n",
    "        dataframe = dataframe.dropna()\n",
    "    \n",
    "    elif feature_name not in dataframe.columns :\n",
    "        \n",
    "        return(\"WRONG INPUT: the feature name is not a column in the dataframe\")\n",
    "    \n",
    "    dataframe = input_dataframe[feature_name].value_counts().reset_index()\n",
    "    count_distinct = len(dataframe)\n",
    "    \n",
    "    # now we de-capitalize all letters\n",
    "    dataframe_decapitalized = pd.DataFrame(index=range(len(dataframe)), columns=[f'decap_{feature_name}'])\n",
    "    for index, datapoint in dataframe.iterrows() :\n",
    "        row = {f'decap_{feature_name}' : datapoint[feature_name].lower()}\n",
    "        dataframe_decapitalized.loc[index] = row\n",
    "        \n",
    "    dataframe_counts = dataframe_decapitalized.value_counts() # distinct decapitalized values and their counts\n",
    "    count_distinct_decap = len(dataframe_counts)\n",
    "    diff_distinct_decap = count_distinct - count_distinct_decap # difference of distinct values number when removing capitalization\n",
    "    \n",
    "    err_low_cap = [] # list to store all de-capitalized values that have a count superior to 0\n",
    "    for value, count in dataframe_counts.items() :\n",
    "        if count > 1 :\n",
    "            err_low_cap.append(value[0])\n",
    "        else :\n",
    "            break\n",
    "            \n",
    "    list_df_diff_cap = [] # list to store all values in initial dataframes that appear with diff capitalizations\n",
    "    for index, datapoint in dataframe.iterrows() :\n",
    "        feature = datapoint[feature_name]\n",
    "        if feature.lower() in err_low_cap : # if the value in initial dataframe appears in previously established list\n",
    "            list_df_diff_cap.append(feature)\n",
    "            \n",
    "    return({\"initial_count\":count_distinct, \"decap_count\":count_distinct_decap, \"diff_distinct_decap\":diff_distinct_decap, \"list_df_diff_cap\":list_df_diff_cap})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd179c8",
   "metadata": {},
   "source": [
    "## Extra spaces\n",
    "\n",
    "Extra spaces in a sentence can be hard to visually detect but two labels with a different number of spaces will  be considered as different strings. Altough it is straightforward to get rid of leading and trailing spaces (with the _.strip_ function in python) and it should be a naturally included procedure in any algorithm, spaces in the middle of the sentences should also be taken into account (_for example, \"9 %\" and \"9%\" are considered to be two different strings, and the .strip function will not help_). \n",
    "\n",
    "<u>The **countExtraSpaces** function</u>:\n",
    "\n",
    "The function takes as input:\n",
    "- a dataframe of elements in the relevant feature column \n",
    "- a column name \n",
    "\n",
    "It returns a dictionary with the following fields:\n",
    "- `initial_count` : the number of distinct values (of the input). note that it takes capitalization into account\n",
    "- `nospace_count` : the number of distinct values once we remove the extra spaces (leading, trailing but also in the middle)\n",
    "- `diff_distinct_nosp` : the number of values that appear several times but with different number of spaces\n",
    "- `list_df_diff_nosp` : a list of all these values that appear several times with different number of spaces, and said values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87531d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countExtraSpaces(input_dataframe, feature_name) :\n",
    "    \n",
    "    # we remove null values\n",
    "    if (len(input_dataframe) != len(input_dataframe.dropna())) :\n",
    "        input_dataframe = input_dataframe.dropna()\n",
    "        \n",
    "    elif feature_name not in dataframe.columns :\n",
    "        \n",
    "        return(\"WRONG INPUT: the feature name is not a column in the dataframe\")\n",
    "        \n",
    "    dataframe = input_dataframe[feature_name].value_counts().reset_index()\n",
    "    count_distinct = len(dataframe.value_counts()) \n",
    "    \n",
    "    # we create a new dataframe to store the values without spaces\n",
    "    dataframe_nospace = pd.DataFrame(index=range(len(dataframe)), columns=[f'noSp_{feature_name}'])\n",
    "    for index, datapoint in dataframe.iterrows() :\n",
    "        row = {f'noSp_{feature_name}' : (\"\").join(datapoint[feature_name].split(\" \"))}\n",
    "        dataframe_nospace.loc[index] = row\n",
    "        \n",
    "    dataframe_counts = dataframe_nospace.value_counts() # distinct values with removed spaces and their counts\n",
    "    count_distinct_nosp = len(dataframe_counts)\n",
    "    diff_distinct_nosp = count_distinct - count_distinct_nosp # difference of distinct values number when removing extra spaces\n",
    "    \n",
    "    err_nospace = [] # list to store all extra-space removed values that have a count superior to 1\n",
    "    for value, count in dataframe_counts.items() :\n",
    "        if count > 1 :\n",
    "            err_nospace.append(value[0])\n",
    "        else :\n",
    "            break\n",
    "            \n",
    "    list_df_diff_nosp = [] # list to store all values in initial dataframes that appear with diff extra-spaces\n",
    "    for index, datapoint in dataframe.iterrows() :\n",
    "        feature = datapoint[feature_name]\n",
    "        if (\"\").join(feature.split(\" \")) in err_nospace : # if the value in initial dataframe appears in previously established list\n",
    "            list_df_diff_nosp.append(feature)\n",
    "            \n",
    "    return({\"initial_count\":count_distinct, \"nospace_count\":count_distinct_nosp, \"diff_distinct_nosp\":diff_distinct_nosp, \"list_df_diff_nosp\":list_df_diff_nosp})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172de70",
   "metadata": {},
   "source": [
    "## Typo detection\n",
    "\n",
    "For the purpose of typo detection, I have written several functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4b80c",
   "metadata": {},
   "source": [
    "<u>The **checkInput** function:</u>\n",
    "\n",
    "Takes as input a dataframe, and returns:\n",
    "- True if the dataframe has only one column of distinct and non-null values\n",
    "- False otherwise\n",
    "\n",
    "<u>The **normalizeDataframe** function:</u>\n",
    "\n",
    "The function takes as input a dataframe and a feature name, and returns a new dataframe of one column, containing the distinct decapitalized values in the feature. \n",
    "\n",
    "This function is used in other functions to flag errors (other than capitalization) without taking letter case into account (_for example, if we did not de-capitalize everything, analysis of letter distribution would make a distinction between capital and lowcase letters_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkInput(dataframe, feature_name) :\n",
    "    \n",
    "    if feature_name not in dataframe.columns :\n",
    "        return((False, \"ERROR: the feature name is not a column in the dataframe\"))\n",
    "    \n",
    "    else :\n",
    "        return ((True, None))\n",
    "    \n",
    "def normalizeDataframe(dataframe, feature_name) :\n",
    "    \n",
    "    # first we de-capitalize all letters\n",
    "    for index, datapoint in dataframe.iterrows() :\n",
    "        dataframe.at[index, feature_name] = str(datapoint[feature_name]).lower()\n",
    "        \n",
    "    # then we take the list of distinct rows\n",
    "    valuecounts = dataframe[feature_name].value_counts().reset_index()\n",
    "    \n",
    "    return pd.DataFrame(valuecounts[feature_name]).dropna() # we return the distinct values without null rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c4d4e",
   "metadata": {},
   "source": [
    "<u>The **LevenshteinRatios** function:</u>\n",
    "\n",
    "It takes as input two strings, and return a dictionnary with the following fields:\n",
    "- `simple_ratio` $r_s$\n",
    "- `token_sort_ratio` $r_{sort}$\n",
    "- `token_set_ratio` $r_{set}$\n",
    "\n",
    "_I did not include the_ partial_ratio _in the computed Levenshtein indices, because it is not relevant to label distinction. Indeed, the partial ratio reflects the ressemblance of <u>substrings</u> in the given strings, which we are not interested in: if two strings should be flagged as typos, then they will have similar length and a high partial ratio, but the other ratios will also reflect that. However, a string might be a substring of another on purpose, both being different labels. That would however be a partial ratio of 100, which is not relevant_.  \n",
    "\n",
    "The <mark>Levenshtein</mark> distance is a string metric (Vladimir Levenshtein, 1965) for measuring the distance or ressemblance between two sequences. It corresponds to the minimum number of actions (being defined as \"insertion\", \"deletion\" or \"substitution\") necessary to change one string into another. The computation of the levenshtein similarity ratio is based on the levenshtein distance.\n",
    "- the simple ratio : calculates the edit distance based on the ordering of both input strings\n",
    "- the token sort ratio : accounts for similar strings without taking order into account, unlike above\n",
    "- the token set ratio : it is similar to the token sort ratio, but it removes common tokens before computing similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LevenshteinRatios(s1, s2) :\n",
    "    \n",
    "    ratios = {}\n",
    "    ratios[\"simple_ratio\"] = fuzz.ratio(s1, s2)\n",
    "    ratios[\"token_sort_ratio\"] = fuzz.token_sort_ratio(s1, s2)\n",
    "    ratios[\"token_set_ratio\"] = fuzz.token_set_ratio(s1, s2)\n",
    "    \n",
    "    return(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5fe28",
   "metadata": {},
   "source": [
    "<u>The **detectTypos** function:</u>\n",
    "\n",
    "The function takes as input:\n",
    "- the input dataframe\n",
    "- the feature name of the column we are interested in\n",
    "- the levenshtein threshold  $t_{levenshtein}$ : minimum value for which we flag two strings as potential typos based on the average of their three computed levenshtein ratios\n",
    "- the length threshold  $t_{length}$ : maximum length difference allowed between two strings to consider them as similar enough. It should be given in percent (_for example, if the length threshold for strings $s_1$ and $s_2$ is 10, where $s_2$ has the maximum length, we will not consider $s_1$ as a potential typo if the length of $s_1$ is less than 90% the length of $s_2$_).\n",
    "- the distance threshold  $t_{distance}$ : maximum length for which we consider two strings for typo detection (in euclidean distance ratio for letter distribution).\n",
    "\n",
    "The four last parameters are given default values.\n",
    "\n",
    "The input dataframe can have several columns, as it is anyway normalized with the **normalizeDataframe** function.\n",
    "\n",
    "The function returns a dataframe, with the string combinations flagged as potential typos:\n",
    "- the two strings\n",
    "- the simple ratio \n",
    "- the token sort ratio\n",
    "- the token set ratio\n",
    "- the norm ratio\n",
    "\n",
    "Note that the returned flagged typos should be manually checked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a47eca",
   "metadata": {},
   "source": [
    "In **detectTypos**, we iterate over all string combinations. In the loop, given two strings $s_1$ and $s_2$, the typo detection is as follows:\n",
    "\n",
    "Let $n_1$ and $n_2$ the length of $s_1$ and $s_2$. We take the two strings as lower case. Then,\n",
    "- we skip the iteration if \n",
    "<center> $ min(n_1, n_2) < (100-t_{length}) \\times max(n_1, n_2)$ </center> \n",
    "\n",
    "_(the strings have too much of a length difference to be the same strings with typos)_\n",
    "\n",
    "- we compute the average \n",
    "<center>$avg_{ratio}=\\frac{r_s + r_{sort} + r_{set}}{3}$ </center>\n",
    "\n",
    "If $avg_{ratio} > t_{levenshtein}$, then we flagg the two strings as typos\n",
    "\n",
    "- We compute $L_1$={$l_a^1, l_b^1, ...$} and $L_2$={$l_a^2, l_b^2, ...$} the frequencies of apparition of the different characters (includes spaces and punctuation) present in both strings ($\\forall$ letter $i$ and $j=1,2$ : $l_i^j \\in R$). \n",
    "\n",
    "Then, we compute $d_1 = \\sqrt{\\sum_{l \\in L_1}l^2}$ and $d_2 = \\sqrt{\\sum_{l \\in L_2}l^2}$, and the set $L_{diff}$ where $\\forall l_{letter} \\in L_{diff}$ : \n",
    "\n",
    "$$ l_{letter} = \n",
    "          \\begin{cases} |l_{letter}^1 - l_{letter}^2| \\; if \\; l_{letter} \\in L_1 \\; and \\; l_{letter} \\in L_2  \n",
    "          \\\\ l_{letter}^1 \\; if \\; l_{letter} \\in L_1 \\; and \\; l_{letter} \\notin L_2\n",
    "          \\\\ l_{letter}^2 \\; if \\; l_{letter} \\notin L_1 \\; and \\; l_{letter} \\in L_2 \n",
    "          \\end{cases} $$\n",
    "          \n",
    "We compute $d = \\sqrt{\\sum_{l \\in L_{diff}}l^2}$. If $d<t_{distance}$, then we flagg the two strings as potential typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05b6b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define constant for typo detection\n",
    "levenshtein_threshold = 90\n",
    "length_threshold = 10 # IN PERCENT\n",
    "distance_threshold = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectTypos(input_dataframe, feature_name, levenshtein_threshold=90, length_threshold=10, distance_threshold=0.15) :\n",
    "    \n",
    "    # first we check the input\n",
    "    check_input = checkInput(input_dataframe, feature_name)\n",
    "    if check_input[0] == False :\n",
    "        return (check_input[1])\n",
    "    \n",
    "    # then we remove null values and capitalization. we only keep distinct values \n",
    "    dataframe = normalizeDataframe(input_dataframe, feature_name)\n",
    "    \n",
    "    n = len(dataframe)\n",
    "    values = dataframe.values\n",
    "        \n",
    "    # we create the return dataframe to store the results (typo suspicion)\n",
    "    typo_suspicion = pd.DataFrame(columns=[\"s1\", \"s2\", \"simple_ratio\", \"token_sort_ratio\", \"token_set_ratio\", \"distance\"])\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n-1) : \n",
    "        for j in range(i+1, n) :\n",
    "            \n",
    "            s1 = values[i][0].lower()\n",
    "            s2 = values[j][0].lower()\n",
    "            \n",
    "            n1, n2 = len(s1), len(s2)\n",
    "            \n",
    "            # first if the length is too different we go to next iteration\n",
    "            if min(n1, n2) < (100-length_threshold)*max(n1, n2)/100 :\n",
    "                continue # we eliminate the possibility of similar strings and we skip this iteration \n",
    "            \n",
    "            # PART 1: LEVENSHTEIN RATIOS\n",
    "            \n",
    "            r_levenshtein = LevenshteinRatios(s1, s2)\n",
    "            avg_lev = sum(r_levenshtein.values())/len(r_levenshtein.values())\n",
    "            \n",
    "            # PART 2: LETTER DISTRIBUTION - this includes spaces\n",
    "            \n",
    "            counts1 = Counter(s1)\n",
    "            counts2 = Counter(s2)\n",
    "            letters1 = set(counts1.keys())\n",
    "            letters2 = set(counts2.keys())\n",
    "            \n",
    "            d1 = np.linalg.norm(list(counts1.values()))\n",
    "            d2 = np.linalg.norm(list(counts2.values()))\n",
    "            \n",
    "            letters_intersect = letters1.intersection(letters2)\n",
    "            letters_union = letters1.union(letters2)\n",
    "            \n",
    "            letters_diff = {}\n",
    "            \n",
    "            for letter in letters_union :\n",
    "                if letter in letters_intersect : # is in both sentence\n",
    "                    letters_diff[letter] = np.abs(counts1[letter]-counts2[letter])\n",
    "                elif letter in letters1 :\n",
    "                    letters_diff[letter] = counts1[letter]\n",
    "                else :\n",
    "                    letters_diff[letter] = counts2[letter]\n",
    "            \n",
    "            dist = np.linalg.norm(list(letters_diff.values()))\n",
    "            dist2 = dist/(d1+d2)\n",
    "            \n",
    "            if avg_lev >= levenshtein_threshold or dist2 <= distance_threshold :\n",
    "                row_typo = {\"s1\": s1, \"s2\": s2, \n",
    "                            \"simple_ratio\":r_levenshtein[\"simple_ratio\"], \n",
    "                            \"token_sort_ratio\": r_levenshtein[\"token_sort_ratio\"], \n",
    "                            \"token_set_ratio\": r_levenshtein[\"token_set_ratio\"], \n",
    "                            \"distance\": dist2}\n",
    "                typo_suspicion.loc[len(typo_suspicion)] = row_typo \n",
    "        \n",
    "    return(typo_suspicion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dee471",
   "metadata": {},
   "source": [
    "**<u>Examples</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[\"If at first you don't succeed, try, try, try again.\"], \n",
    "        [\"If at first you don't succeed try try try again\"], \n",
    "        [\"i have always depended on the kindness of strangers,\"], \n",
    "        [\"I have always depended on the kindness of strangers.\"], \n",
    "        [\"I have always depended the kindness of strangers.\"],\n",
    "         [\"I have always depended the kindness of stranger.\"],\n",
    "        [\"I hav alwaïz dipended on ze kindnesse of strangeurs.\"]]\n",
    "df_ex2 = pd.DataFrame(data2, columns=['quote'])\n",
    "detectTypos(df_ex2, \"quote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d75f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"If at first you don't succeed, try, try, try again.\"], \n",
    "        [\"If at first you don't succeed try try try again\"], \n",
    "        [\"i have always depended on the kindness of strangers\"], \n",
    "        [\"I have always depended on the kindness of strangers.\"], \n",
    "        [\"I have always depended the kindness of strangers.\"], \n",
    "        [\"Cogito ergo sum\"], \n",
    "        [\"Cogito ergo sam\"], \n",
    "        [\"Well done is better than well said\"], \n",
    "        [\"Well said is better than well done\"], \n",
    "        [\"Nobody puts Baby in a corner.\"], \n",
    "        [\"If at first you don't succeed, try, try again.\"]]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df_ex = pd.DataFrame(data, columns=['quote'])\n",
    "\n",
    "d = detectTypos(df_ex, \"quote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee4881",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66fcd9",
   "metadata": {},
   "source": [
    "The function **textStatistics** detects in case a string feature has a value where its length really lies outside of the lengths of the overall column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textStatistics(dataframe, feature_name, lb = 25, ub = 75) :\n",
    "    \n",
    "    if feature_name not in dataframe.columns :\n",
    "        return((False, \"ERROR: the feature name is not a column in the dataframe\"))\n",
    "    \n",
    "    lengths = [len(feature) for feature in dataframe[feature_name]]\n",
    "    \n",
    "    avg = np.average(lengths)\n",
    "    minimum = min(lengths)\n",
    "    maximum = max(lengths)\n",
    "    \n",
    "    print(avg, minimum, maximum)\n",
    "    \n",
    "    lengths_array = np.array(lengths)\n",
    "\n",
    "    q1 = np.percentile(lengths_array, lb) # lb-th percentile\n",
    "    q3 = np.percentile(lengths_array, ub) # ub-th percentile\n",
    "    iqr = q3 - q1 # interquartile range\n",
    "\n",
    "    # Define the outlier boundaries\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    outliers = lengths_array[(lengths_array < lower_bound) | (lengths_array > upper_bound)]\n",
    "    \n",
    "    plt.hist(lengths, bins=20)\n",
    "\n",
    "    return(\"Outliers:\", outliers.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c845634",
   "metadata": {},
   "source": [
    "The function **outliers(dataframe, feature_name)** takes as input a dataframe and the name of a numerical feature, and returns the values that lie outside of the feature's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(dataframe, feature_name, lb = 25, ub = 75) :\n",
    "    \n",
    "    if feature_name not in dataframe.columns :\n",
    "        return((False, \"ERROR: the feature name is not a column in the dataframe\"))\n",
    "    \n",
    "    data = dataframe[feature_name].tolist()\n",
    "    \n",
    "    avg = np.average(data)\n",
    "    minimum = min(data)\n",
    "    maximum = max(data)\n",
    "    \n",
    "    q1 = np.percentile(data, lb)\n",
    "    q2 = np.percentile(data, ub)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q2 - 1.5 * iqr\n",
    "    \n",
    "    outliers = data[ (data < lower_bound) | (data < upper_bound) ]\n",
    "    \n",
    "    return(\"Outliers:\", outliers.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
